{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leKfIqFPgjXk"
      },
      "source": [
        "# Nilearn tutorial\n",
        "This optional notebook is about the amazing [nilearn](https://nilearn.github.io/) Python package for applying statistical learning techniques (from GLMs to multivariate \"decoding\" and connectivity techniques) to neuroimaging data. In addition, it features all kinds of neat functionality like automic fetching of publicly available data, (interactive) visualization of brain images, and easy image operations.\n",
        "\n",
        "In this tutorial, we'll walk you through the basic of the package's functionality in a step-by-step fashion. Notably, this notebook contains several exercises (which we call \"ToDos\"), which are meant to make this tutorial more interactive! Also, this tutorial is merely an introduction to (pars of) the Nilearn package. We strongly recommend checking out the excellent [user guide](https://nilearn.github.io/user_guide.html) and [example gallery](https://nilearn.github.io/auto_examples/index.html) on the Nilearn website if you want to delve deeper into the package's (more advanced) features.\n",
        "\n",
        "### Contents\n",
        "1. What is Nilearn?\n",
        "2. Data formats\n",
        "3. Data visualization\n",
        "4. Image manipulation\n",
        "5. Region extraction\n",
        "6. Connectome/connectivity analyses\n",
        "\n",
        "**Estimated time needed to complete**: 1-3 hours (depending on your experience with Python)<br>\n",
        "**Credits**: if you end up using `nilearn` in your work, please cite the corresponding [article](https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4j8I_15bgjXm",
        "outputId": "171e2053-c1a6-43f2-e3b6-8e272c953de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.11.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.1)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nilearn) (24.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.14.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (4.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Downloading nilearn-0.11.1-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.11.1\n"
          ]
        }
      ],
      "source": [
        "# Let's see whether Nilearn is installed\n",
        "try:\n",
        "    import nilearn\n",
        "except ImportError:\n",
        "    # if not, install it using pip\n",
        "    !pip install nilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz7WeqmHgjX4"
      },
      "source": [
        "## 1. What is Nilearn?\n",
        "Nilearn is one of the packages in the growing [\"nipy\" ecosystem](https://nipy.org/) of Python packages for neuroimaging analysis (see also [MNE](https://mne.tools/stable/index.html), [nistats](https://nistats.github.io/), [nipype](https://nipype.readthedocs.io/en/latest), [nibabel](https://nipy.org/nibabel/), and [dipy](http://dipy.org/)). Specifically, Nilearn provides tools for analysis techniques like functional connectivity, multivariate (machine-learning based) \"decoding\", but also more \"basic\" tools like image manipulation and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtDIzMXTgjX4"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Go through the <a href='https://nilearn.github.io/'>Nilearn</a> website to get an idea of what functionality the package offers. Also, for more information, check out <a href='https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full'>this article</a> about \"machine learning for neuroimaging with scikit-learn\", which discusses some of Nilearn's functionality in more detail.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X31ieBQogjX4"
      },
      "source": [
        "On Nilearn's website, you can see that the package contains several modules (such as `connectome`, `datasets`, `decoding`, etc.). In the following sections, we will discuss some of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjg8qwKgjX4"
      },
      "source": [
        "## 2. Data formats\n",
        "Nilearn's functionality implicitly assumes that your MRI data is stored in nifti images. Many functions in Nilearn accept either strings pointing towards the path of a nifti file (or a list with multiple paths) or a `Nifti1Image` object from the `nibabel` package. Together, these two types of inputs (filenames pointing to nifti files and `nibabel` `Nifti1Images`) are often referred to a \"niimgs\" (or \"niimg-like\") by Nilearn &mdash; a term you'll see a lot in Nilearn's documentation (for more info about data formats in Nilearn, see [this explainer](https://nilearn.github.io/manipulating_images/input_output.html)).\n",
        "\n",
        "Before we go on, let's actually download some example nifti files to work with. Fortunately, Nilearn has an entire module dedicated to fetching example data and other useful files (e.g., atlases): [nilearn.datasets](https://nilearn.github.io/manipulating_images/input_output.html#datasets). We'll import it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r7PQeWzBgjX5"
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqYEcLmQgjX5"
      },
      "source": [
        "Now, we will download some example data from the famous [Haxby et al.](https://science.sciencemag.org/content/293/5539/2425) (2001) study. This can be done using the `fetch_haxy` function from the `datasets` module. We'll download data from only a single subject for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_qAfYEETgjX5",
        "outputId": "8c029ba8-0a99-46d3-d05f-b70ff767e6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_add_readme_to_default_data_locations\u001b[0m\u001b[1;34m]\u001b[0m Added README.md to \u001b[35m/root/\u001b[0m\u001b[95mnilearn_data\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_add_readme_to_default_data_locations</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Added README.md to <span style=\"color: #800080; text-decoration-color: #800080\">/root/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nilearn_data</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset created in \u001b[35m/root/nilearn_data/\u001b[0m\u001b[95mhaxby2001\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset created in <span style=\"color: #800080; text-decoration-color: #800080\">/root/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">haxby2001</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m Downloading data from \u001b[4;94mhttps://www.nitrc.org/frs/download.php/7868/mask.nii.gz\u001b[0m \u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloading data from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.nitrc.org/frs/download.php/7868/mask.nii.gz</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m  \u001b[33m...\u001b[0mdone. \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m seconds, \u001b[1;36m0\u001b[0m min\u001b[1m)\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span>  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>done. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> seconds, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> min<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m Downloading data from \u001b[4;94mhttp://data.pymvpa.org/datasets/haxby2001/MD5SUMS\u001b[0m \u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloading data from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://data.pymvpa.org/datasets/haxby2001/MD5SUMS</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m  \u001b[33m...\u001b[0mdone. \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m seconds, \u001b[1;36m0\u001b[0m min\u001b[1m)\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span>  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>done. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> seconds, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> min<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m Downloading data from \u001b[4;94mhttp://data.pymvpa.org/datasets/haxby2001/subj1-2010.01.14.tar.gz\u001b[0m \u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloading data from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://data.pymvpa.org/datasets/haxby2001/subj1-2010.01.14.tar.gz</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_chunk_report_\u001b[0m\u001b[1;34m]\u001b[0m Downloaded \u001b[1;36m80084992\u001b[0m of \u001b[1;36m314803244\u001b[0m bytes \u001b[1m(\u001b[0m\u001b[1;36m25.4\u001b[0m%%,    \u001b[1;36m2.\u001b[0m9s remaining\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_chunk_report_</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80084992</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">314803244</span> bytes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.4</span>%%,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>9s remaining<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_chunk_report_\u001b[0m\u001b[1;34m]\u001b[0m Downloaded \u001b[1;36m152887296\u001b[0m of \u001b[1;36m314803244\u001b[0m bytes \u001b[1m(\u001b[0m\u001b[1;36m48.6\u001b[0m%%,    \u001b[1;36m2.\u001b[0m1s remaining\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_chunk_report_</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">152887296</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">314803244</span> bytes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.6</span>%%,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>1s remaining<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_chunk_report_\u001b[0m\u001b[1;34m]\u001b[0m Downloaded \u001b[1;36m215064576\u001b[0m of \u001b[1;36m314803244\u001b[0m bytes \u001b[1m(\u001b[0m\u001b[1;36m68.3\u001b[0m%%,    \u001b[1;36m1.\u001b[0m4s remaining\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_chunk_report_</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">215064576</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">314803244</span> bytes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.3</span>%%,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>4s remaining<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_chunk_report_\u001b[0m\u001b[1;34m]\u001b[0m Downloaded \u001b[1;36m281100288\u001b[0m of \u001b[1;36m314803244\u001b[0m bytes \u001b[1m(\u001b[0m\u001b[1;36m89.3\u001b[0m%%,    \u001b[1;36m0.\u001b[0m5s remaining\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_chunk_report_</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">281100288</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">314803244</span> bytes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">89.3</span>%%,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>5s remaining<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m  \u001b[33m...\u001b[0mdone. \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m seconds, \u001b[1;36m0\u001b[0m min\u001b[1m)\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span>  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>done. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> seconds, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> min<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34muncompress_file\u001b[0m\u001b[1;34m]\u001b[0m Extracting data from \n",
              "\u001b[35m/root/nilearn_data/haxby2001/9cbdfe22144f858ab0d4958f8162e296/\u001b[0m\u001b[95msubj1-2010.01.14.tar.gz...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">uncompress_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Extracting data from \n",
              "<span style=\"color: #800080; text-decoration-color: #800080\">/root/nilearn_data/haxby2001/9cbdfe22144f858ab0d4958f8162e296/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">subj1-2010.01.14.tar.gz...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34muncompress_file\u001b[0m\u001b[1;34m]\u001b[0m .. done.\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">uncompress_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> .. done.\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# This might take a while, depending on your internet speed\n",
        "data = datasets.fetch_haxby(\n",
        "    data_dir=None,\n",
        "    subjects=1,\n",
        "    fetch_stimuli=False,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DubvE2hgjX5"
      },
      "source": [
        "<div class='alert alert-info'>\n",
        "    <b>Note</b>: throughout this tutorial, you might see several warnings (in red) after running cells. The code is most likely executing perfectly fine (they are not errors!) and are often caused by other packages that Nilearn uses internally.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzaEFsGUgjX5"
      },
      "source": [
        "The `fetch_haxby` function returns a dictionary with the location of the downloaded files and some metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vEAS9fksgjX5",
        "outputId": "c87eb41b-c907-4fa1-ca33-4e55c38f6cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'anat': ['/root/nilearn_data/haxby2001/subj1/anat.nii.gz'],\n",
            " 'description': '.. _haxby_dataset:\\n'\n",
            "                '\\n'\n",
            "                'Haxby dataset\\n'\n",
            "                '=============\\n'\n",
            "                '\\n'\n",
            "                'Access\\n'\n",
            "                '------\\n'\n",
            "                'See :func:`nilearn.datasets.fetch_haxby`.\\n'\n",
            "                '\\n'\n",
            "                'Notes\\n'\n",
            "                '-----\\n'\n",
            "                'Results from a classical :term:`fMRI` study that investigated '\n",
            "                'the differences between\\n'\n",
            "                'the neural correlates of face versus object processing in the '\n",
            "                'ventral visual\\n'\n",
            "                'stream. Face and object stimuli showed widely distributed and '\n",
            "                'overlapping\\n'\n",
            "                'response patterns.\\n'\n",
            "                '\\n'\n",
            "                'See :footcite:t:`Haxby2001`.\\n'\n",
            "                '\\n'\n",
            "                'Content\\n'\n",
            "                '-------\\n'\n",
            "                'The \"simple\" dataset includes:\\n'\n",
            "                \"    :'func': Nifti images with bold data\\n\"\n",
            "                \"    :'session_target': Text file containing run data\\n\"\n",
            "                \"    :'mask': Nifti images with employed mask\\n\"\n",
            "                \"    :'session': Text file with condition labels\\n\"\n",
            "                '\\n'\n",
            "                'The full dataset additionally includes\\n'\n",
            "                \"    :'anat': Nifti images with anatomical image\\n\"\n",
            "                \"    :'func': Nifti images with bold data\\n\"\n",
            "                \"    :'mask_vt': Nifti images with mask for ventral \"\n",
            "                'visual/temporal cortex\\n'\n",
            "                \"    :'mask_face': Nifti images with face-reponsive brain \"\n",
            "                'regions\\n'\n",
            "                \"    :'mask_house': Nifti images with house-reponsive brain \"\n",
            "                'regions\\n'\n",
            "                \"    :'mask_face_little': Spatially more constrained version \"\n",
            "                'of the above\\n'\n",
            "                \"    :'mask_house_little': Spatially more constrained version \"\n",
            "                'of the above\\n'\n",
            "                '\\n'\n",
            "                'References\\n'\n",
            "                '----------\\n'\n",
            "                '\\n'\n",
            "                '.. footbibliography::\\n'\n",
            "                '\\n'\n",
            "                'For more information see:\\n'\n",
            "                'PyMVPA provides a tutorial using this dataset :\\n'\n",
            "                'http://www.pymvpa.org/tutorial.html\\n'\n",
            "                '\\n'\n",
            "                'More information about its structure :\\n'\n",
            "                'http://dev.pymvpa.org/datadb/haxby2001.html\\n'\n",
            "                '\\n'\n",
            "                '\\n'\n",
            "                'License\\n'\n",
            "                '-------\\n'\n",
            "                'unknown\\n',\n",
            " 'func': ['/root/nilearn_data/haxby2001/subj1/bold.nii.gz'],\n",
            " 'mask': '/root/nilearn_data/haxby2001/mask.nii.gz',\n",
            " 'mask_face': ['/root/nilearn_data/haxby2001/subj1/mask8b_face_vt.nii.gz'],\n",
            " 'mask_face_little': ['/root/nilearn_data/haxby2001/subj1/mask8_face_vt.nii.gz'],\n",
            " 'mask_house': ['/root/nilearn_data/haxby2001/subj1/mask8b_house_vt.nii.gz'],\n",
            " 'mask_house_little': ['/root/nilearn_data/haxby2001/subj1/mask8_house_vt.nii.gz'],\n",
            " 'mask_vt': ['/root/nilearn_data/haxby2001/subj1/mask4_vt.nii.gz'],\n",
            " 'session_target': ['/root/nilearn_data/haxby2001/subj1/labels.txt']}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint  # useful function to \"pretty print\" dictionaries\n",
        "pprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqZisN3tgjX6"
      },
      "source": [
        "Let's inspect the `\"description\"` in more detail, which described the contents of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-ZV3yrsVgjX6",
        "outputId": "475f9313-260e-4342-be48-0dc0b33adb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _haxby_dataset:\n",
            "\n",
            "Haxby dataset\n",
            "=============\n",
            "\n",
            "Access\n",
            "------\n",
            "See :func:`nilearn.datasets.fetch_haxby`.\n",
            "\n",
            "Notes\n",
            "-----\n",
            "Results from a classical :term:`fMRI` study that investigated the differences between\n",
            "the neural correlates of face versus object processing in the ventral visual\n",
            "stream. Face and object stimuli showed widely distributed and overlapping\n",
            "response patterns.\n",
            "\n",
            "See :footcite:t:`Haxby2001`.\n",
            "\n",
            "Content\n",
            "-------\n",
            "The \"simple\" dataset includes:\n",
            "    :'func': Nifti images with bold data\n",
            "    :'session_target': Text file containing run data\n",
            "    :'mask': Nifti images with employed mask\n",
            "    :'session': Text file with condition labels\n",
            "\n",
            "The full dataset additionally includes\n",
            "    :'anat': Nifti images with anatomical image\n",
            "    :'func': Nifti images with bold data\n",
            "    :'mask_vt': Nifti images with mask for ventral visual/temporal cortex\n",
            "    :'mask_face': Nifti images with face-reponsive brain regions\n",
            "    :'mask_house': Nifti images with house-reponsive brain regions\n",
            "    :'mask_face_little': Spatially more constrained version of the above\n",
            "    :'mask_house_little': Spatially more constrained version of the above\n",
            "\n",
            "References\n",
            "----------\n",
            "\n",
            ".. footbibliography::\n",
            "\n",
            "For more information see:\n",
            "PyMVPA provides a tutorial using this dataset :\n",
            "http://www.pymvpa.org/tutorial.html\n",
            "\n",
            "More information about its structure :\n",
            "http://dev.pymvpa.org/datadb/haxby2001.html\n",
            "\n",
            "\n",
            "License\n",
            "-------\n",
            "unknown\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(data['description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RvOVblgjX6"
      },
      "source": [
        "Alright, now we have some data to work with. With the `image` module in Nilearn, we can load in and perform many different operations on nifti images. We'll import it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NKzrLwdagjX6"
      },
      "outputs": [],
      "source": [
        "from nilearn import image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH52cRjBgjX6"
      },
      "source": [
        "And let's load in the anatomical image from the Haxby dataset that we downloaded using the `load_img` function from the `image` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H44c4GmKgjX6",
        "outputId": "60470bab-6210-4fd8-c95d-c431684c1d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The variable anat_img is an instance of the Nifti1Image class!\n"
          ]
        }
      ],
      "source": [
        "anat_img = image.load_img(data['anat'])\n",
        "print(\"The variable anat_img is an instance of the %s class!\" % type(anat_img).__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWhyUjJugjX6"
      },
      "source": [
        "Note that `load_img` is basically the same as the `nibabel` function `load`, but with some extra functionality (like loading in a list of files using [wildcards](https://techterms.com/definition/wildcard)) and checks (like whether it's really a nifti image)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYh5-mmUgjX6"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: In the <tt>subj1</tt> subdirectory of the download directory, there are multiple nifti files with \"masks\" outlining functional regions for this particular subject. Can you load them all at once using the <tt>load_img</tt> function with a wildcard? Store the result in a variable named <tt>all_mask_imgs</tt>, which should be a 4D <tt>Nifti1Image</tt> object. Don't forget to remove the <tt>NotImplementedError</tt>!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97e2b1da5c382cc32ab98bd34dfbfb33",
          "grade": false,
          "grade_id": "cell-f5ae4fde30b94961",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gzcOBcNugjX6",
        "outputId": "96c4203d-9ef4-4fba-dcf1-c62b4153e3d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f71ffa0f506a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Try it below!\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1e0a3446f2ddfe2ecc1df73e2b62714a",
          "grade": true,
          "grade_id": "cell-2e1a1059eda6dc6a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "if8DiW7GgjX6",
        "outputId": "f9604a52-5834-4bb5-ea7a-d20d649a451d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_mask_imgs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a1669a8cd388>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check shape (should be 5 volumes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_mask_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Well done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_mask_imgs' is not defined"
          ]
        }
      ],
      "source": [
        "''' Tests the ToDo above. '''\n",
        "\n",
        "# Check shape (should be 5 volumes)\n",
        "assert(all_mask_imgs.shape == (40, 64, 64, 5))\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFOv8L_TgjX6"
      },
      "source": [
        "In general, Nilearn contains several functions that can be seen as \"wrappers\" around common operations that you'd normally use `nibabel` and/or `numpy` for, such as creating new Nifti images from numpy arrays (`image.new_img_like`), indexing (4D) images (`image.index_img`), and averaging 4D images across time (`image.mean_img`). For example, suppose we have a 3D numpy array containing, e.g., $\\hat{\\beta}$ values from a GLM analysis with the same shape as our anatomical scan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JQEibytgjX6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "arr_3d = np.random.normal(0, 1, size=anat_img.shape)\n",
        "print(\"The variable arr_3d is an instance of the %s class!\" % type(arr_3d).__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCm5Fx5_gjX6"
      },
      "source": [
        "Now, suppose we want to convert this data to a `nibabel` `Nifti1Image` object, so that we can perform other operations on it or visualize it (using Nilearn). We can use the `new_img_like` function for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nFn62K7gjX6"
      },
      "outputs": [],
      "source": [
        "arr_3d_img = image.new_img_like(ref_niimg=anat_img, data=arr_3d)\n",
        "print(\"The variable arr_3d_img is an instance of the %s class!\" % type(arr_3d_img).__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62nfmFGgjX6"
      },
      "source": [
        "Before we go into more detail, let's make it a little bit more interesting by focusing on the data visualization tools within Nilearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlc5UtpkgjX6"
      },
      "source": [
        "## 3. Data visualization\n",
        "The data visualization tools in Nilearn are grouped in the `plotting` module. The `plotting` module, in our opinion, is one of Nilearn's most useful features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5XKIfVMgjX6"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Before going on, browse through the Nilearn <a href=\"https://nilearn.github.io/plotting/index.html\">documentation of the plotting module</a> to get a feel for what's possible.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKZMu-DcgjX7"
      },
      "source": [
        "Alright, let's start by importing the module (and telling Jupyter to plot our figures inside the notebook using `%matplotlib inline`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA3ea_t6gjX7"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKpIpk2gjX7"
      },
      "source": [
        "We can try plotting our `anat_img` (high-resolution T1-weighted image) using the `plot_anat` function. In the most basic approach, you can simply call it with your image (a `Nifti1Image` or path to a nifti file) as the first argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pj6SxU_gjX7"
      },
      "outputs": [],
      "source": [
        "display = plotting.plot_anat(anat_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlI2fB1FgjX7"
      },
      "source": [
        "Note that the variable `display` is an instance of a custom Nilearn class (`Orthoslicer`) which contains some nifty (pun intended) features as well (which we won't discuss here, but check out Nilearn's [plotting reference](https://nilearn.github.io/plotting/index.html)).\n",
        "\n",
        "But Nilearn plotting functions contain many (optional) arguments that you can use to customize your plot. For example, the `display_mode` argument allows you to plot the image in one (or more) particular dimensions (e.g., the \"X\" axis, which is usually sagittal) and the `cut_coords` argument allows you to specify the number (if integer) or locations of the particular slices/cuts (if list):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZjXhlYRgjX7"
      },
      "outputs": [],
      "source": [
        "display = plotting.plot_anat(anat_img, display_mode='x', cut_coords=[-40, -20, 0, 20, 40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp8esUE-gjX7"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Read through the <a href=\"https://nilearn.github.io/modules/generated/nilearn.plotting.plot_anat.html#nilearn.plotting.plot_anat\">documentation</a> for the <tt>plot_anat</tt> function. Now, try to make the following plot of the <tt>anat_img</tt> image: 8 cuts in the \"y\" direction, thresholded at 50, a dimming factor of -1, and a title \"T1-weighted image\".\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1b76d886c3a6b747ba712305723c2795",
          "grade": true,
          "grade_id": "cell-aa0e623948fa6082",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JxXZE7VtgjX7"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizDC_JzgjX7"
      },
      "source": [
        "There are many other plotting functions besides `plot_anat`, which we'll highlight when relevant in the next sections of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5KCSVrugjX7"
      },
      "source": [
        "## 4. Image manipulation\n",
        "Nilearn contains a lot of functionality to easily manipulate images. Note that, again, all of this could be implemented with `numpy` (in fact, Nilearn uses `numpy` \"under the hood\" for many of its operations); just see the Nilearn functions as \"wrappers\" around common numpy routines for nifti-based arrays (which also handle the updating of image affines, e.g. after resampling an image, appropriately).\n",
        "\n",
        "To showcase some `nilearn` functions, we'll use the functional MRI data we downloaded (`bold.nii.gz`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDyZxdsmgjX-"
      },
      "outputs": [],
      "source": [
        "func_img = image.load_img(data['func'][0])\n",
        "print(\"Shape of functional MRI image: %s\" % (func_img.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpf80xRrgjX-"
      },
      "source": [
        "This file, however, is quite large (~400 MB), as it contains concatenated data across 12 runs. To limit the amount of data that we need to load into RAM, we'll only load in the data from the first run. We can find out which volumes belong to which run in the \"session_target\" information, which we'll load in below as a [pandas dataframe](https://pandas.pydata.org/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUACK2kSgjX-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "metadata = pd.read_csv(data['session_target'][0], sep=' ')\n",
        "print(\"Shape of metadata dataframe: %s\" % (metadata.shape,), end='\\n\\n')\n",
        "print(metadata.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0lyVz2pgjX-"
      },
      "source": [
        "Here, \"chunks\" refer to the specific run index and \"labels\" refers to the timepoint-by-timepoint condition (i.e., the condition of the active block at each moment in time, e.g., images of scissors, images of chairs, rest blocks, etc.). Let's compute the number of timepoints in the first \"chunk\" (run):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzr-Zmk6gjX-"
      },
      "outputs": [],
      "source": [
        "nvol_run_1 = np.sum(metadata['chunks'] == 0)\n",
        "print(\"Number of volumes in run 1: %i\" % nvol_run_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W08sO8JigjX-"
      },
      "source": [
        "Alright, now we can use Nilearn's `index_img` function from the `image` module to index our `func_img` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt07zLf4gjX-"
      },
      "outputs": [],
      "source": [
        "to_index = np.arange(nvol_run_1, dtype=int)\n",
        "func_img_run1 = image.index_img(func_img, to_index)\n",
        "print(\"Shape of func_img_run1: %s\" % (func_img_run1.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yjslyyZgjX-"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: with the information in the header of <tt>func_img_run1</tt>, can you determine how long (in seconds) the first run lasted? Store the answer in a variable named <tt>length_run1</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0120bf1ec4c36ef0a49edc9d4f422803",
          "grade": false,
          "grade_id": "cell-1a1f44978d9c5162",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gtBevMrZgjX-"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0593a0c2610aab01c05ad7cb2ab3fb3b",
          "grade": true,
          "grade_id": "cell-787331677b47a81a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sC-DQWkDgjX-"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "assert(length_run1 == 302.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl2pE5y-gjX-"
      },
      "source": [
        "Okay, that was the boring part. Let's do some more interesting things!\n",
        "\n",
        "### 4.1. Image mathematics\n",
        "Nilearn provides some functions to make your life easier when doing array mathematics on 3D or 4D images. For example, the `mean_func` from the `image` module computes the mean across time for every voxel in a 4D image. (Note that, again, this could also be done in `numpy`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipTeVbCDgjX-"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Use the <tt>mean_img</tt> function from the <tt>image</tt> module to average across the volumes of our <tt>func_img_run1</tt> image and store it in a new variable named <tt>mean_func</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ac0e8175d1b4a46e01bfa528cef6ee0f",
          "grade": false,
          "grade_id": "cell-b2ea2a87e46d774c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "h6xbOg5wgjX-"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "230a369caf18d27a934241577ec9dba6",
          "grade": true,
          "grade_id": "cell-02f346bd565ef93a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "jBHZEIufgjX-"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "assert(mean_func.shape == (40, 64, 64))\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YrmJkKgjX-"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Now, plot the <tt>mean_func</tt> image using the <tt><a href=\"https://nilearn.github.io/modules/generated/nilearn.plotting.plot_epi.html#nilearn.plotting.plot_epi\">plot_epi</a></tt> function from the <tt>plotting</tt> module. Use whatever arguments to make the plot as pretty as you like.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e84babbefda610ae997334830f22179b",
          "grade": true,
          "grade_id": "cell-f5f6a1b870dfacb6",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "J51F6WNpgjX-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcLIQXcJgjX-"
      },
      "source": [
        "Another useful function from the `image` module is `math_img`. This function allows you to perform more complex  mathematical operations to entire images at once. For example, suppose you want to mean-center the time series of every voxel $v$ in an image (i.e., subtract the mean across time from each time point).\n",
        "\n",
        "We can do that as follows using `math_img`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dUgZf44PgjX-",
        "outputId": "3b506b47-ec40-4a0d-f3c4-aba27af25cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'func_img_run1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7d44a5689520>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_centered_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img - np.mean(img, axis=3, keepdims=True)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_img_run1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'func_img_run1' is not defined"
          ]
        }
      ],
      "source": [
        "mean_centered_img = image.math_img('img - np.mean(img, axis=3, keepdims=True)', img=func_img_run1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJQ3IkRgjX-"
      },
      "source": [
        "As you can see, the function `math_img` takes a string indicating a particular (numpy) operation on the variable \"img\", which is given as an argument to the function. You can give as many extra arguments (associated with particular images) to the function as you'd like. For example, the above operation can also be performed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8p-A-bWgjX-"
      },
      "outputs": [],
      "source": [
        "mean_centered_img = image.math_img('img_4d - img_mean[:, :, :, np.newaxis]', img_4d=func_img_run1, img_mean=mean_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKbw1RgOgjX-"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Compute the voxelwise TSNR (mean across time divided by standard deviation across time for each voxel) of the <tt>func_img_run1</tt> image using <tt>math_img</tt> and store it in a variable <tt>tsnr_func</tt>. Then, plot the image using <tt>plot_epi</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f965cb2377abcad11d479c3c813fdb19",
          "grade": false,
          "grade_id": "cell-f61bcf2545810c9f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7teC29kWgjX-"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbe2e92a47f79e85ba04967c0301f685",
          "grade": true,
          "grade_id": "cell-deca03e9c3b43e5b",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IW8iV4qLgjX-"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "np.testing.assert_almost_equal(\n",
        "    tsnr_func.get_data()[20, 30, 30],\n",
        "    64.76949,\n",
        "    decimal=5\n",
        ")\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFGQWDnkgjX-"
      },
      "source": [
        "### 4.2. Image preprocessing\n",
        "Nilearn also provides some functionality for basic preprocessing of (functional) images. Note the adjective \"basic\" &mdash; most preprocessing steps (such as image registration, motion correction, susceptibility distortion correction, etc.) should be done using other packages (for which we strongly recommend [Fmriprep](https://fmriprep.readthedocs.io/)). That said, Nilearn does provide some preprocessing functionality, such as smoothing (with `image.smooth_img`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yZgyECWbgjX-",
        "outputId": "8bf1a834-c0cf-4bf1-bd60-616386fd5b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tsnr_func' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-af1a73356964>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsnr_func_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsnr_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwhm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_epi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsnr_func_smooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tsnr_func' is not defined"
          ]
        }
      ],
      "source": [
        "tsnr_func_smooth = image.smooth_img(tsnr_func, fwhm=10)\n",
        "display = plotting.plot_epi(tsnr_func_smooth);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUODw29mgjX_"
      },
      "source": [
        "Hmm, perhaps it would be nicer to plot a thresholded version of this map on the subject's high-resolution T1-weighted scan ... Of course, Nilearn has a function for that: `plot_stat_map`, which takes both a \"stat_map\" (which can be anything, as long as it's a 3D image) and a background image, as well as an optional threshold argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OymWw97gjX_"
      },
      "outputs": [],
      "source": [
        "display = plotting.plot_stat_map(tsnr_func_smooth, bg_img=anat_img, threshold=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfvimtcgjX_"
      },
      "source": [
        "Note that you can even create interactive image viewers using, for example, the `view_img` function. This works especially well in Jupyter notebooks. Importantly (at least in Jupyter notebooks), you should call this plotting function at the end of the code cell and you should not assign the output of the function to a new variable, otherwise the viewer won't render."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_01LGbvngjX_",
        "outputId": "6e64c086-ff16-4dc9-8646-9c0b620785c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotting' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-acd50b278d65>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do not add code after this line, otherwise it won't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsnr_func_smooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manat_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plotting' is not defined"
          ]
        }
      ],
      "source": [
        "# Do not add code after this line, otherwise it won't work\n",
        "plotting.view_img(tsnr_func_smooth, bg_img=anat_img, threshold=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUurzHewgjX_"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Try moving the crosshairs in the image!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAjH0ac0gjX_"
      },
      "source": [
        "While these interactive viewers are amazing, be careful not to open too many of them, as they need quite a bit of memory to run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC6FIFIZgjX_"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Time for a slightly harder ToDo. The <tt>masking</tt> module of Nilearn contains a function <tt>compute_epi_mask</tt> (see <a href=\"https://nilearn.github.io/modules/generated/nilearn.masking.compute_epi_mask.html#nilearn.masking.compute_epi_mask\">docs</a>), which computes a binary brain mask (similar to FSL's <tt>bet</tt>) on a 3D functional image (usually the mean functional image). Do this for our <tt>mean_func</tt> image (store it in a variable named <tt>func_mask</tt>) and plot the mask on top of the <tt>mean_func</tt> image (i.e., the background) using the <tt>plot_roi</tt> function from the <tt>plotting</tt> module.<br>\n",
        "    \n",
        "Don't forget to import the <tt>masking</tt> module!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b4b2b739b6b8f1535dbbaf78aa63f20",
          "grade": false,
          "grade_id": "cell-2f230da34ddb4593",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3izxLoO8gjX_"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d4c031663ec6ba6cbf82f89d4ee628b8",
          "grade": true,
          "grade_id": "cell-7302654c72f554a2",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "qWDJCIjlgjX_"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "assert(func_mask.get_data().sum() == 23594)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay-ZpqEQgjX_"
      },
      "source": [
        "In addition to spatial smoothing and creating (functional) brain masks, Nilearn actually also provides some tools for preprocessing in the time domain (such as detrending,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0utQnxKgjX_"
      },
      "source": [
        "### 4.3. Image masking\n",
        "A common operation in fMRI analyses is *masking*: extracting particular voxels from the entire dataset, usually based on a binary brain mask (like you computed in the previous ToDo). Masking, at least in fMRI analyses, is often done on the spatial dimensions of 4D images; as such, masking can be seen as a operation that takes in a 4D image with spatial dimensions $X \\times Y \\times Z$ and temporal dimension $T$ and returns a $T \\times K$ 2D array, where $K$ is the number of voxels that \"survived\" (for lack of a better word) the masking procedure.\n",
        "\n",
        "Reasons to mask your data could be, for example, to exclude non-brain voxels (like in skullstripping) or to perform confirmatory region-of-interest (ROI) analyses, or to extract one or multiple \"seed regions\" for connectivity analyses.\n",
        "\n",
        "Nilearn provides several functions and classes that perform masking, which differ in how extensive they are (some only perform masking on a single image, others do this for multiple images at the same time, and/or may additionally perform preprocessing steps). Importantly, all take in a 4D niimg-like object and return a 2D *numpy array*.\n",
        "\n",
        "We'll first take a look at the most simple and low-level implementation: `apply_mask`. This function takes in a 4D image (which will be masked), a *binary* 3D image (i.e., with only zeros and ones, where ones indicate that they should be included) as mask, and optionally a smoothing kernel size (FWHM in millimeters) and returns a masked 2D array. Let's do this for our data (`func_img_run1`) using the brain mask (`func_mask`) you computed earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGeqWrUDgjX_"
      },
      "outputs": [],
      "source": [
        "from nilearn import masking\n",
        "\n",
        "# Let's compute the epi mask again, in case you didn't do this in the previous ToDo\n",
        "func_mask = masking.compute_epi_mask(mean_func)\n",
        "\n",
        "print(\"Before masking, our data has shape %s ...\" % (func_img_run1.shape,))\n",
        "func_masked = masking.apply_mask(func_img_run1, func_mask)\n",
        "print(\"... and afterwards our data has shape %s and is a %s\" % (func_masked.shape, type(func_masked).__name__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLsHmzejgjX_"
      },
      "source": [
        "Importantly, we can \"undo\" the masking operation by the complementary `unmask` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIwbQTCXgjX_"
      },
      "outputs": [],
      "source": [
        "func_unmasked = masking.unmask(func_masked, func_mask)\n",
        "print(\"func_unmasked is a %s and has shape %s\" % (type(func_unmasked).__name__, func_unmasked.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrW0vE6MgjX_"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Similar to what we did before, mask the <tt>func_img_run1</tt> file with the <tt>func_mask</tt> file. Then, compute for every voxel its TSNR (with numpy, because we're dealing with a numpy array after masking!) and set all voxels with a TSNR lower than 100 to 0. Then, unmask the data again and save it (i.e., the <tt>Nifti1Image</tt>) in a variable named <tt>tsnr_masked_img</tt>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fd6758bf573f623f55f8df6def0d4f25",
          "grade": false,
          "grade_id": "cell-91f37f5f52229d4c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "go0cpmVegjX_"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "54e35a5bfe973b53fd471f7720c1b3f2",
          "grade": true,
          "grade_id": "cell-fff249fecf6b3373",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "V86d9cZFgjX_"
      },
      "outputs": [],
      "source": [
        "'''Tests the above ToDo. '''\n",
        "tmp_test = tsnr_masked_img.get_data()\n",
        "\n",
        "np.testing.assert_array_equal(\n",
        "    tmp_test[30, 30, 30, :5],\n",
        "    np.array([1695., 1712., 1690., 1695., 1711.])\n",
        ")\n",
        "\n",
        "np.testing.assert_array_equal(\n",
        "    tmp_test[20, 30, 30, :],\n",
        "    np.zeros(tmp_test.shape[-1])\n",
        ")\n",
        "\n",
        "print('Well done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnbpRws7gjX_"
      },
      "source": [
        "Apart from the `apply_mask` and `unmask` functions, Nilearn also contains a more extensive \"masking\" class, `NiftiMasker` (from the `input_data` module), that has some extra preprocessing features. Unlike the name suggests, this class does much more than masking: it also (optionally) allows you to spatially and temporally preprocess your data! It works slightly differently than the relatively simple functions we have discussed so far though, so we'll spend a little time on its \"mechanics\".\n",
        "\n",
        "We'll start with importing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkDuiOQCgjX_"
      },
      "outputs": [],
      "source": [
        "from nilearn.input_data import NiftiMasker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-85MgVmgjX_"
      },
      "source": [
        "Importantly, `NiftiMasker` is *not* a function, but a (custom) *class*. With this class, you can create (or \"initialize\") a new object of this class. Upon inialization, you can give it particular arguments that define how the `NiftiMasker` object will behave. Read through the [documentation](https://nilearn.github.io/modules/generated/nilearn.input_data.NiftiMasker.html#nilearn.input_data.NiftiMasker) of the `NiftiMasker` class to see which arguments it accepts.\n",
        "\n",
        "For now, we'll initialize a very simple `NiftiMasker` that only accepts a particular brain mask (and set `verbose=True` to print some extra information)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MiiOtq6gjX_"
      },
      "outputs": [],
      "source": [
        "masker = NiftiMasker(mask_img=func_mask, verbose=True)\n",
        "print(\"The masker variable is a %s object\" % type(masker).__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNFHhSTBgjX_"
      },
      "source": [
        "Realize that we haven't masked anything, yet! We can do this using the `fit` and `transform` methods of `NiftiMasker` objects. This \"design\" (i.e., custom objects with `fit` and `transform` methods that implement data transformations) is based on the design principles of the [scikit-learn](https://scikit-learn.org/stable/) Python package for machine learning (some of the authors of the Nilearn package also created and contribute to the `scikit-learn` package).\n",
        "\n",
        "Now, after initialization of the `NiftiMasker` object, you need to call the `fit` method, which needs the to-be-transformed (here: masked) 4D image as an input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6402wfQ-gjX_"
      },
      "outputs": [],
      "source": [
        "masker.fit(func_img_run1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxmnV01vgjX_"
      },
      "source": [
        "In the output block of the cell above (`Out[xx]`), you can see that the `fit` function returns \"itself\" (i.e., the `NiftiMasker` object). Importantly, any computations are happening \"in-place\", so the output (i.e., the object itself) does not need to be stored in a new variable.\n",
        "\n",
        "Moreover, because we set `verbose=True`, some extra information about what the `NiftiMasker` does is printed (first, the data from `func_img_run1` is loaded in memory and, second, the `func_mask` is resampled to the same space as the `func_img_run1` if necessary).\n",
        "\n",
        "Again, even after calling `fit`, our 4D data not been masked yet! The fitting procedure only makes sure that the `NiftiMasker` object is ready to transform whatever image you want (later, when you call `transform`). You might think, \"why do you need a `fit` method, if it is not actually 'fitting' anything?\" In this example, it is indeed redundant, but there are many other features in the `NiftiMasker` class that actually perform some computation. For example, we could initialize a `NiftiMasker` object without an explicit `mask_img`, but with the argument `mask_strategy='epi'`, which will actually compute a functional brain mask when you will call `fit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j-nGxrdWgjYA",
        "outputId": "6e32da2a-1aca-40ae-fec1-06de693b2112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NiftiMasker' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1bb6ce4bc5a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmasker2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNiftiMasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmasker2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_img_run1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NiftiMasker' is not defined"
          ]
        }
      ],
      "source": [
        "masker2 = NiftiMasker(mask_strategy='epi', verbose=1)\n",
        "masker2.fit(func_img_run1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B08F2qTigjYA"
      },
      "source": [
        "Here, you see that the computation of the mask is happening when we called `fit`. In general, this design pattern &mdash; which defers any computation to a particular method (here: `fit`) and uses a different method (e.g., `transform`) for the actual transformation &mdash; is ideal for *cross-validation*. Cross-validation is a procedure where you want to use separate subsets of your data for *estimating* (fitting) parameters or operations and actually *applying* those parameters or operations, which is common in, for example, machine learning.\n",
        "\n",
        "Anyway, we are digressing. After calling `fit`, we can now call the `transform` method with the image that we want to transform (here: mask) as input, which will return the masked image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZIn88SBgjYA"
      },
      "outputs": [],
      "source": [
        "# would be the same for the masker2 object\n",
        "masked_file = masker.transform(func_img_run1)\n",
        "print(\"\\nThe variable masked_file is an instance of the %s class,\" % type(masked_file).__name__)\n",
        "print(\"with shape %s\" % (masked_file.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yZALfDLgjYA"
      },
      "source": [
        "Note that `transform` again need the to-be-transformed image (here: `func_img_run1`) as input, but this could have been any other 4D image (as long as it has the same dimensions)!\n",
        "\n",
        "To make your life a little easier, `NiftiMasker` objects also contain another function, `fit_transform`, that (guess what) combines the `fit` and `transform` methods in a single method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb5OdMN1gjYA"
      },
      "outputs": [],
      "source": [
        "masked_file = masker.fit_transform(func_img_run1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnuVSjM8gjYA"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: One of the extra preprocessing features that <tt>NiftiMasker</tt> objects contain is spatial smoothing. Create a new <tt>NiftiMasker</tt> object that will smooth the <tt>func_img_run1</tt> image (with a FWHM of 7 mm.) as well as mask it using the \"epi\" strategy. Store the result in a new variable named <tt>smooth_and_masked_file</tt>. Check the <a href=\"https://nilearn.github.io/modules/generated/nilearn.input_data.NiftiMasker.html#nilearn.input_data.NiftiMasker\">docs</a> for more information.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4c48829e2fad608f84e96ab2f437208b",
          "grade": false,
          "grade_id": "cell-2122b0d1ed7d5830",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "vLDQUeXKgjYA"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4e733a13cf7f3b2560bf476ce68d6541",
          "grade": true,
          "grade_id": "cell-724a50e2096fad73",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "m3C62YEggjYA"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "np.testing.assert_array_almost_equal(\n",
        "    smooth_and_masked_file[0, :5],\n",
        "    np.array([782.5649 , 849.7041 , 775.19464, 824.16406, 847.68823]),\n",
        "    decimal=4\n",
        ")\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDqYF1NYgjYA"
      },
      "source": [
        "You might think that this \"workflow\" using `NiftiMasker` objects is a bit cumbersome. For very simple operations, it is (relative to just using, e.g., the `apply_mask` function), but the initialize-fit-transform pattern allows for easy integration with other common (machine-learning related) transformations as implemented in the `scikit-learn` package (which we won't discuss here, but check [this page](https://nilearn.github.io/building_blocks/manual_pipeline.html) for more information).\n",
        "\n",
        "We'll take a look at the more extensive functionality (including temporal preprocessing) of `NiftiMasker` at the end of the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igZQi-xPgjYA"
      },
      "source": [
        "### 4.4. Temporal preprocessing\n",
        "Nilearn contains some basic temporal preprocessing functionality, such as ...\n",
        "* detrending (removing a linear trend);\n",
        "* standardization (normalizing the signal with mean 0 and standard deviation 1);\n",
        "* high- and low-pass filtering;\n",
        "* generic confound removal (using linear regression)\n",
        "\n",
        "Again, there are different interfaces for these operations. The most \"basic\" ones are `signal.clean` and `image.clean_img`, which are very similar except for that `signal.clean` works on 2D numpy arrays while `image.clean_img` work on 4D \"niimg-like\" objects. Also, these temporal preprocessing operations can be done using `NiftiMasker` objects.\n",
        "\n",
        "For now, we'll focus on the `image.clean_img` interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-kZTGRegjYA"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Read through the <a href=\"https://nilearn.github.io/modules/generated/nilearn.image.clean_img.html#nilearn.image.clean_img\">documentation</a> of the <tt>clean_img</tt> function.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV9oKKHugjYA"
      },
      "source": [
        "As you can see, the function has a mandatory argument, `imgs`, referring to the to-be-cleaned image (or images, if it's a list of 4D images) and several optional arguments referring to the different preprocessing options. Below, we'll use the function to detrend, standardize, and high-pass (but not low-pass) the `func_img_run1` data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVFb_QCzgjYA"
      },
      "outputs": [],
      "source": [
        "# Note that high_pass should be defined in Hz (here: 1/100) and\n",
        "# the t_r (time to repetition) parameter is necessary for the high-pass filter\n",
        "\n",
        "# This may take a couple of seconds!\n",
        "func_clean = image.clean_img(func_img_run1, detrend=True, standardize=True, high_pass=0.01, t_r=2.5)\n",
        "print(\"func_clean is an instance of the %s class!\" % type(func_clean).__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paou3afygjYA"
      },
      "source": [
        "Additionally, the `clean_img` (and `signal.clean`) also allow you to filter out confounds such as motion parameters, physiological traces (e.g., RETROICOR traces), or data-derived noise sources such as the timecourses from \"high-variance\" voxels).\n",
        "\n",
        "To get some example confound variables, let's actually extract the timecourses of \"high-variance\" voxels within our data (`func_img_run1`), which can be done using the [image.high_variance_confounds](https://nilearn.github.io/modules/generated/nilearn.image.high_variance_confounds.html#nilearn.image.high_variance_confounds) function. Subsequently, we can regress out these timecourses (confounds) from our data using `image.clean_img`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmYvenUgjYA"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Read through the <a href=\"https://nilearn.github.io/modules/generated/nilearn.image.high_variance_confounds.html#nilearn.image.high_variance_confounds\">docs</a> of the <tt>high_variance_confounds</tt> function. Then, extract 10 time courses from the highest-variance voxels and store this in a new variable named <tt>hvar_confs</tt>. Make sure to actually use the <tt>func_mask</tt> variable as a mask during the high-variance confound estimation (see the <tt>mask_img</tt> argument) to exclude voxels outside the brain. Lastly, regress out the confounds from the <tt>func_img_run1</tt> image (using the argument <tt>confounds</tt>). Make sure to also detrend, standardize, and high-pass (with 0.01 Hz) the data. Store the cleaned data in a variable named <tt>func_clean</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "45e213f226d71c0efa88dda3e5480f34",
          "grade": false,
          "grade_id": "cell-758a93b2bb74db58",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "G1q1iRibgjYA"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here (might take a couple of seconds to run)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8260814d121b12ccc6e7f428ac725d5c",
          "grade": true,
          "grade_id": "cell-62a4be1b42cba61e",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vhQ1EkY5gjYA"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "np.testing.assert_array_almost_equal(\n",
        "    func_clean.get_data()[30, 30, 30, :5],\n",
        "    np.array([-0.12187857, 1.3470134, -0.70243245, -0.442845, 0.9848256]),\n",
        "    decimal=5\n",
        ")\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkDeOAbFgjYA"
      },
      "source": [
        "As mentioned before, the `NiftiMasker` also allows you to temporally preprocess your data (including confound removal)! Note that in the `NiftiMasker` implementation the `confounds` (2D numpy array with confound time series) should be given as an argument during `fit` (not as an argument during initialization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVwDQqb7gjYA"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Try to redo the last ToDo exercise, but this time using the <tt>NiftiMasker</tt> implementation. Store the cleaned image (which should be a 2D numpy array) in a new variable named <tt>func_clean2</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6c6ee914a8261c54a5cc340619e36356",
          "grade": false,
          "grade_id": "cell-ca57f097e20fdef3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8pOWI0cDgjYA"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here (might take a couple of seconds to run)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51a9f938a61751ead10b0532e3972a3b",
          "grade": true,
          "grade_id": "cell-b1a4258b2fb01fa7",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "j94wpq_tgjYA"
      },
      "outputs": [],
      "source": [
        "''' Tests the ToDo above. '''\n",
        "np.testing.assert_array_almost_equal(\n",
        "    func_clean2[:5, 5000],\n",
        "    np.array([ 0.04785081,  0.4619908 ,  0.88480663, -0.01620996, -0.29138982]),\n",
        "    decimal=5\n",
        ")\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMoJznZgjYA"
      },
      "source": [
        "## 5. Region extraction\n",
        "A common operation in fMRI analyses is to reduce the dimensionality of the data by restricting your analyses to one or more regions-of-interest (ROIs), which may be either functionally or anatomically (using atlases) defined. While this is, technically, a form of masking (and could have been discussed in section 4.3), we wanted to discuss this topic in a separate section as Nilearn has a dedicated module &mdash; [regions](https://nilearn.github.io/modules/reference.html#module-nilearn.regions) &mdash; for region extraction.\n",
        "\n",
        "Before we go into more detail, we need some other data, because the Haxby data is in \"native\" functional space, but anatomical atlases are usually defined in some standard space (usually a variant of the MNI152 space). Below, we'll load in data from a single subject from a study by [Hilary Richardson and colleagues](https://www.nature.com/articles/s41467-018-03399-2) (2018), in which participants watched the same short movie. Note that the data was preprocessed already using the [Fmriprep](https://fmriprep.readthedocs.io/en/stable/) software package. Again, we can use a function from the `datasets` module (`fetch_development_fmri`) to download the data. Also, we'll remove our previously defined variables to clear up some memory by the \"magic\" command `% reset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk746forgjYA"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "\n",
        "# And redefine imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from nilearn import datasets, image, masking, plotting\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52JzDFkXgjYA"
      },
      "outputs": [],
      "source": [
        "data = datasets.fetch_development_fmri(n_subjects=1, reduce_confounds=True, age_group='adult')\n",
        "pprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_jCSmxgjYA"
      },
      "source": [
        "Let's take a look at the description in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxb9qGSpgjYA"
      },
      "outputs": [],
      "source": [
        "print(data['description'].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbVDqU64gjYA"
      },
      "source": [
        "The advantage of this dataset (for our purposes) is that the functional data is already aligned to the MNI152 template. We'll first load in the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2_v1kXIgjYB"
      },
      "outputs": [],
      "source": [
        "func_img = image.load_img(data['func'][0])\n",
        "print(\"Shape of func_img: %s\" % (func_img.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcyjoF5ngjYB"
      },
      "source": [
        "Note that the functional data, as expected, is a 4D image (with 168 volumes). We can verify that it's aligned by plotting the (by Nilearn computed) brain mask on top of the MNI template (which is the default background image in the `plot_roi` function):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2wzCQTQgjYB"
      },
      "outputs": [],
      "source": [
        "func_mean = image.mean_img(func_img)\n",
        "display = plotting.plot_roi(func_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l3lUPhQgjYB"
      },
      "source": [
        "Note that the voxel dimensions of our `func_img` data ($50 \\times 59 \\times 50$) are different from the standard MNI (2mm) template ($91 \\times 109 \\times 91$) &mdash; how, then, is Nilearn able to plot these images on top of each other? This is because, \"under the hood\", Nilearn resamples our data (`func_img`) to the dimensions of the MNI template (using the function [image.resample_to_img](https://nilearn.github.io/modules/generated/nilearn.image.resample_to_img.html#nilearn.image.resample_to_img)). We'll take a closer look at this function later in this tutorial.\n",
        "\n",
        "Anyway, it seems that our functional data aligns quite well with the MNI template (apart from some signal dropout in inferior temporal and orbitofrontal cortex, which is normal).\n",
        "\n",
        "### Intermezzo: surface plots\n",
        "Because our data is aligned to standard MNI152 space, we can use another cool visualization feature from Nilearn: surface plots! Nilearn contains the resampling parameters (as available from the [Freesurfer](https://surfer.nmr.mgh.harvard.edu/) software package) necessary to resample volumetric images in MNI space to the corresponding \"fsaverage\" surface space. To do so, you can use the `view_img_on_surf` function, which takes a volumetric image (`img`) and projects it on a corresponding surface (`surf`).\n",
        "\n",
        "We'll use the \"fsaverage5\" `surf_mesh` specifically, because it is in a slightly lower resolution than the default \"fsaverage\" space (saving some memory):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmLGz7ZRgjYB"
      },
      "outputs": [],
      "source": [
        "plotting.view_img_on_surf(\n",
        "    stat_map_img=func_mean,\n",
        "    surf_mesh='fsaverage5'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVFkH3b6gjYB"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Check out different views of the surface by dragging it left/right/up/down with your mouse and zooming in (with the scroll wheel of your mouse or your trackpad). Also try switching hemispheres and surface type (Inflated vs. Pial) using the buttons on the bottom of the plot.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Bke6_sgjYB"
      },
      "source": [
        "Note that you could also project your data on the subject's own surface reconstruction (instead of \"fsaverage\") if you have that (and assuming whatever data you want to project is actually aligned to your subject's high-resolution T1-weighted scan)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C23I6EqgjYB"
      },
      "source": [
        "### 5.1. \"Labels\" vs. \"maps\" atlas images\n",
        "In Nilearn, there is a distinction between \"atlas maps images\" and \"atlas label images\".\n",
        "\n",
        "#### 5.1.1. Atlas label images\n",
        "In atlas label images, there is only a single image containing different integer \"labels\" ($1, 2, 3, ... R$) corresponding to different brain regions within a particular atlas. Importantly, these different regions do not overlap in atlas label images.\n",
        "\n",
        "We'll download and load in such an atlas below, the \"maxprob\" version of the Harvard-Oxford Cortical atlas (using `datasets.fetch_atlas_harvard_oxford`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWyGdxy7gjYB"
      },
      "outputs": [],
      "source": [
        "from nilearn import regions\n",
        "ho_maxprob_atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKmUgb1VgjYB"
      },
      "source": [
        "Note that the Harvard-Oxford atlas is, technically, a probabilistic atlas, where each voxel (often) is assigned a set of probabilities of belonging to different region (e.g., voxel $x$ may belong with 80% \"certainty\" to the right amygdala and 20% \"certainty\" to the right hippocampus). Here, however, we loaded in the \"maxprob\" version, which, for each voxel, assigns the region with the highest probability.\n",
        "\n",
        "Here, the `ho_maxprob_atlas` variable is a dictionary with the keys \"labels\" and \"maps\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsqO8a2SgjYB"
      },
      "outputs": [],
      "source": [
        "pprint(ho_maxprob_atlas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk3w9IwXgjYB"
      },
      "source": [
        "Let's load in the actual atlas image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REMhrBAzgjYB"
      },
      "outputs": [],
      "source": [
        "ho_maxprob_atlas_img = image.load_img(ho_maxprob_atlas['maps'])\n",
        "print(\"ho_maxprob_atlas_img is a 4D image with shape %s\" % (ho_maxprob_atlas_img.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucglZNFcgjYB"
      },
      "source": [
        "The actual values of this map are integers that indicate which region the corresponding voxels belong to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0g1KDk9gjYB"
      },
      "outputs": [],
      "source": [
        "region_int_labels = np.unique(ho_maxprob_atlas_img.get_data())\n",
        "n_regions = region_int_labels.size\n",
        "\n",
        "print(\"There are %i different regions in the Harvard-Oxford cortical atlas!\" % n_regions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgYV5uXtgjYB"
      },
      "source": [
        "But how do we know which number belongs to which region? Basically, the *index* of the labels (in `ho_maxprob_atlas['labels']`) correspond to the values in the map (`ho_maxprob_atlas_img`). For example, the value \"2\" in the atlas map corresponds to the third (remember, Python is 0-indexed) label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELGACUTvgjYB"
      },
      "outputs": [],
      "source": [
        "idx = 2\n",
        "region_with_value2 = ho_maxprob_atlas['labels'][idx]\n",
        "print(\"The region with value 2 is: %s\" % region_with_value2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ALenDYegjYB"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Which region belongs to the value \"10\" in the atlas map? Check the cell above with <tt>pprint(ho_maxprob_atlas)</tt> to verify your answer.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZCkVXyWgjYB"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Can you compute how many voxels the \"Insular cortex\" consists of within this atlas? Hint: you need to load the atlas map (<tt>ho_maxprob_atlas_img</tt>) into memory (using the <tt>get_data</tt> method) for this. Store the answer (an integer) in a new variable named <tt>nvox_insula</tt>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c757817f230148d8d373a740941613a5",
          "grade": false,
          "grade_id": "cell-09f1a08c840e142e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Z-YwhLe-gjYB"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e008bd537ab4313ea8428a604c25bdae",
          "grade": true,
          "grade_id": "cell-d3d54cbb43af8290",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Vf07Sa9DgjYB"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "assert(int(nvox_insula) == 2341)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg-FOyiJgjYB"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Now, let's try it the other way around! What value in our atlas map belongs to the region \"Occipital Fusiform Gyrus\"? Try it programatically (i.e., without counting the regions in <tt>ho_maxprob_atlas['labels']</tt>. Store the value in a new variable named <tt>value_ofg</tt>. Hint: perhaps you can use the list method <a href=\"https://www.programiz.com/python-programming/methods/list/index\">index</a>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "99e171920d7c196d7285ef0a47d09111",
          "grade": false,
          "grade_id": "cell-7a752a5f664d849b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x5v9BJ0sgjYB"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "92d209b3b9fb23862292a88952d4a788",
          "grade": true,
          "grade_id": "cell-767021f7795edf78",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SsKwxnKbgjYB"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo'''\n",
        "assert(value_ofg == 40)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eTUwp14gjYB"
      },
      "source": [
        "We can actually plot the atlas easily using the function `plot_roi`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYqvFI36gjYC"
      },
      "outputs": [],
      "source": [
        "display = plotting.plot_roi(ho_maxprob_atlas_img, colorbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTflJ9YgjYC"
      },
      "source": [
        "#### 5.1.2. Atlas maps images\n",
        "In atlas maps images, the regions are not defined by single labels (such as in probabilistic atlases) and may overlap. An example of such as atlas map image is the original *probabilistic* version of the Harvard-Oxford atlas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHiTzDbZgjYC"
      },
      "outputs": [],
      "source": [
        "ho_prob_atlas = datasets.fetch_atlas_harvard_oxford('cort-prob-2mm')\n",
        "pprint(ho_prob_atlas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9R74siPgjYC"
      },
      "source": [
        "Again, this atlas (contained in the variable `ho_prob_atlas`) contains both *labels* (`ho_prob_atlas['labels']`) and *maps* (`ho_prob_atlas['maps']`), like with the \"maxprob\" atlas, but this time, the *maps* image is a 4D image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-qjv2LMgjYC"
      },
      "outputs": [],
      "source": [
        "ho_prob_atlas_img = image.load_img(ho_prob_atlas['maps'])\n",
        "print(\"ho_prob_atlas_img has shape %s\" % (ho_prob_atlas_img.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCCwCqn9gjYC"
      },
      "source": [
        "In this `ho_prob_atlas_img`, the fourth dimension now refers to the different regions! Note that there are only 48 volumes because the \"background\" does not get it's own volume. Now, suppose that I would like to extract the volume corresponding to the insula (label nr. 3), we need to extract the *first* volume (this is because the background did get its own volume; a bit confusing, we know):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05P17xW-gjYC"
      },
      "outputs": [],
      "source": [
        "insula_prob_roi = image.index_img(ho_prob_atlas_img, 1)\n",
        "print(\"Shape of insula ROI: %s\" % (insula_prob_roi.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2iK3FwEgjYC"
      },
      "source": [
        "Again, we can plot this using `plot_roi`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XUxsmOzgjYC"
      },
      "outputs": [],
      "source": [
        "display = plotting.plot_roi(insula_prob_roi, cmap='autumn', vmin=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I7Qkso0gjYC"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Because the insula ROI (<tt>insula_prob_roi</tt>) is in MNI152 space, you can also visualize it in a surface plot. Try to use the <tt>view_img_on_stat</tt> function to do so (with, e.g., a threshold of 20).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5c3424f80fadfeb13c89725233787591",
          "grade": false,
          "grade_id": "cell-f00decaa0ebaf730",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fn0h7GfjgjYC"
      },
      "outputs": [],
      "source": [
        "# Plot the insula ROI on the fsaverage5 surface here\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE_GYDG5gjYC"
      },
      "source": [
        "Alright, back to volume plots. Nilearn contains even a function to plot the entire probabilitic atlas (given some threshold) in a single plot: `plot_probabilistic_atlas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EszLqi2hgjYC",
        "outputId": "0b125589-e541-4427-8964-bc8750277c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plotting' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-55dea4c04009>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This may take a couple of seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_prob_atlas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mho_prob_atlas_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolorbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plotting' is not defined"
          ]
        }
      ],
      "source": [
        "# This may take a couple of seconds\n",
        "display = plotting.plot_prob_atlas(ho_prob_atlas_img, colorbar=True, threshold=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfxY-1ApgjYC"
      },
      "source": [
        "At this moment, we could use our insula ROI (after binarizing) to index our functional data (`func_img`), except that we have one problem: although the ROI (`insula_prob_roi`) and our data (`func_img`) are aligned, they do not have the same dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOP_lJjagjYC"
      },
      "outputs": [],
      "source": [
        "print(\"Affine of func_img:\")\n",
        "print(func_img.affine)\n",
        "\n",
        "print(\"\\nAffine of ROI:\")\n",
        "print(insula_prob_roi.affine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC1ssbhxgjYC"
      },
      "source": [
        "Again, Nilearn comes to the rescue! We can use `image.resample_img` (or alternatively, `image.resample_to_img`) to resample our ROI to the resolution of our functional data. Check out the [docs](https://nilearn.github.io/modules/generated/nilearn.image.resample_img.html) of the `resample_img` function!\n",
        "\n",
        "(Note that we didn't have to do this when we plotted our insula ROI on top of an MNI image background, because most plotting functions in Nilearn automatically resample the to-be-plotted data to the background image \"under the hood\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ZGAl2JgjYC"
      },
      "source": [
        "<div class='alert alert-info'>\n",
        "    <b>ToThink</b>: Why do you think we choose to resample our ROI to our data and not the other way around (which is also perfectly possible)? Think about practical reasons!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A5dD3f7gjYC"
      },
      "source": [
        "Let's do this below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWDiVliagjYC"
      },
      "outputs": [],
      "source": [
        "insula_prob_roi_resamp = image.resample_img(\n",
        "    insula_prob_roi,\n",
        "    target_affine=func_img.affine,\n",
        "    target_shape=func_img.shape[:3]\n",
        ")\n",
        "\n",
        "print(\"New affine of ROI:\")\n",
        "print(insula_prob_roi_resamp.affine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0b6onF_gjYC"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Alright, time for a slightly more extensive ToDo! Try the following:<br>\n",
        "\n",
        "<ul>\n",
        "    <li>\n",
        "        Extract the probabilistic ROI of the \"Temporal Pole\" from the <tt>ho_prob_atlas_img</tt> variable (plot it         with <tt>plot_roi</tt> to see whether you succeeded);\n",
        "    </li>\n",
        "    <li>\n",
        "        Using the function <a href=\"https://nilearn.github.io/modules/generated/nilearn.image.threshold_img.html\">image.threshold_img</a>,   \n",
        "        threshold the probabilistic ROI at 40 (setting all voxels with the value 40 and below to zero);\n",
        "    </li>\n",
        "    <li>\n",
        "        Resample the thresholded mask to the dimensions of our functional data (<tt>func_img</tt>);\n",
        "    </li>\n",
        "    <li>\n",
        "        Using <tt>image.math_img</tt>, binarize the thresholded and resampled ROI (where voxels belonging to the\n",
        "        thresholded ROI should have value 1 and 0 otherwise);\n",
        "    <li>\n",
        "        Finally, apply this mask to the <tt>func_img</tt> variable using <tt>apply_mask</tt> (or\n",
        "        <tt>NiftiMasker</tt>, up to you);\n",
        "    </li>\n",
        "    <li>\n",
        "        Average the time series signal across the 1043 temporal pole voxel and store these values (should be an\n",
        "        array of 176 values) in a new variable named <tt>average_tp_signal</tt>.\n",
        "</ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6d1b436b6a0ba41b7c549dc508ecc052",
          "grade": false,
          "grade_id": "cell-9f6880cf82d8b4c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "lg0OxzBqgjYC"
      },
      "outputs": [],
      "source": [
        "# Implement your ToDo here!\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6e4952aac3126a09493c83d5f460922c",
          "grade": true,
          "grade_id": "cell-d5dd2fc31b492a73",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "s80DWeoLgjYC"
      },
      "outputs": [],
      "source": [
        "''' Tests the above ToDo. '''\n",
        "np.testing.assert_array_almost_equal(\n",
        "    average_tp_signal[:5],\n",
        "    np.array([299.29, 299.42, 299.26, 299.89, 298.79]),\n",
        "    decimal=2\n",
        ")\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxY-28LZgjYC"
      },
      "source": [
        "If you did the previous ToDo, you noticed it took quite some lines of code to implement (but way less that you'd need when implementing it in pure Numpy!). This would take even more lines of code if you would like to do this for multiple ROIs, for example, in \"connectome\"-based analyses (which we'll discuss in a bit). Fortunately, Nilearn contains several functions do this within a single line of code.\n",
        "\n",
        "There are different functions for \"atlas maps images\" (such as for our probabilistic Harvard-Oxford atlas) and \"atlas label images\" (such as our \"maxprob\" Harvard-Oxford atlas). The corresponding functions that allow multi-ROI masking and averaging the signal across voxels (like you did in the previous ToDo) are `img_to_signals_labels` and `img_to_signals_maps` from the `regions` module, respectively. Both transform a 4D ($X \\times Y \\times Z \\times T$) image to a 2D ($T \\times K$, where $K$ is the number of regions in the atlas) numpy array.\n",
        "\n",
        "Let's take a look at the `img_to_signals_labels` function first. We'll use our previously defined `ho_maxprob_atlas_img`. Importantly, we first need to resample the atlas label image to the space of our functional data (we'll use `resample_to_img` this time, which is a little less \"verbose\" than the `resample_img` function):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXZpx8dWgjYC"
      },
      "outputs": [],
      "source": [
        "ho_maxprob_atlas_img_resamp = image.resample_to_img(\n",
        "    ho_maxprob_atlas_img,\n",
        "    target_img=func_img,\n",
        "    interpolation='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpPCaJ9RgjYC"
      },
      "source": [
        "Note that we specify that the resampling function should use \"nearest neighbor\" interpolation instead of the default \"continuous\" interpolation (otherwise labels at the edge of regions could get a value of, e.g., 3.05, while labels in atlas label images should always be whole numbers/integers).\n",
        "\n",
        "Now, in a single line of code (`using_, we can transform our 4D image to a 2D array with the average time series (i.e., across voxels) for all ROIs in our atlas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbWGvBMWgjYC"
      },
      "outputs": [],
      "source": [
        "av_roi_data = regions.img_to_signals_labels(\n",
        "    func_img,\n",
        "    labels_img=ho_maxprob_atlas_img_resamp,\n",
        "    background_label=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkrEp-JEgjYC"
      },
      "source": [
        "Notably, the `img_to_signals_labels` returns two things:\n",
        "1. The actual average ROI signals;\n",
        "2. The corresponding (integer) labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caXKSeErgjYD"
      },
      "outputs": [],
      "source": [
        "av_roi_signals = av_roi_data[0]\n",
        "roi_labels = av_roi_data[1]\n",
        "print(\"average_roi_signals is a %s with shape: %s\" % (type(av_roi_signals).__name__, av_roi_signals.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOILH9HTgjYD"
      },
      "source": [
        "The function for atlas maps images (`img_to_signals_maps`) can be used in essentially the same way as the `img_to_signals_labels` function, except that it should be given a atlas maps image instead.\n",
        "\n",
        "By the way, there exists variations of the `NiftiMasker` class that basically does the same as the `img_to_signals_{labels,maps}` functions: `NiftiLabelsMasker` and `NiftiMapsMasker`. Just like the `img_to_signals_labels` function, this indexes our functional data with multiple ROIs in which the signal is subsequently averaged across voxels, but it also includes resampling of the atlas (if necessary) and optional preprocessing (just like the `NiftiMasker` class).\n",
        "\n",
        "We'll show you how it can be used on our data below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtdDlDdegjYD"
      },
      "outputs": [],
      "source": [
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "\n",
        "nlm = NiftiLabelsMasker(labels_img=ho_maxprob_atlas_img)\n",
        "av_roi_signals = nlm.fit_transform(func_img)\n",
        "print(\"Shape of av_roi_signals: %s\" % (av_roi_signals.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCTQ1_rhgjYD"
      },
      "source": [
        "Alright, now you know how to leverage the basic functionality from the `regions` module. Note that, in addition to the several existing anatomically and functionally defined atlases (check out the [datasets](https://nilearn.github.io/modules/reference.html#module-nilearn.datasets) module), Nilearn also contains several wrapper functions for `scikit-learn` functions that allow you to estimate parcellations from your *own* data (using, e.g., [Ward clustering](https://nilearn.github.io/modules/generated/nilearn.regions.Parcellations.html#nilearn.regions.Parcellations), [Kmeans clustering](https://nilearn.github.io/modules/generated/nilearn.regions.Parcellations.html#nilearn.regions.Parcellations), [Dictionary learning](https://nilearn.github.io/modules/generated/nilearn.decomposition.DictLearning.html#nilearn.decomposition.DictLearning) or [canonical ICA](https://nilearn.github.io/modules/generated/nilearn.decomposition.CanICA.html#nilearn.decomposition.CanICA)). But that's something for another tutorial perhaps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq5Zt9GfgjYD"
      },
      "source": [
        "## 6. Connectome/connectivity analyses\n",
        "When we have a 2D array with average time series of multiple ROIs, we can easily implement a \"[connectome](https://en.wikipedia.org/wiki/Connectome)\"-based analysis using the [connectome](https://nilearn.github.io/modules/reference.html#module-nilearn.connectome) module of Nilearn. This module has several functions/classes that allow you to estimate \"functional connectomes\", which are basically connectivity matrices based on a similarity measure (such as correlation) between time series across ROIs. As such, these matrices have a $K \\times K$ shape (where $K$ reflects the number of ROIs). From these connectivity matrices, in turn, you could for example perform [network analyses](https://www.frontiersin.org/articles/10.3389/fncom.2014.00051/full).\n",
        "\n",
        "There are different classes for connectome estimation, including `ConnectivityMeasure` (a general-purpose connectivity estimator), `GroupSparseCovariance` and `GroupSparseCovarianceCV` (estimators for specific \"sparse\" connectivity matrices across multiple subjects).\n",
        "\n",
        "For now, we'll focus on the general `ConnectivityMeasure` class. We'll start by importing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNTOlKThgjYD"
      },
      "outputs": [],
      "source": [
        "from nilearn.connectome import ConnectivityMeasure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-8dNVrgjYD"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Read through the <a href=\"https://nilearn.github.io/modules/generated/nilearn.connectome.ConnectivityMeasure.html#nilearn.connectome.ConnectivityMeasure\">docs</a> of the <tt>ConnectivityMeasure</tt> class.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzooVAzWgjYD"
      },
      "source": [
        "As you can see in the docs, the `ConnectivityMeasure` class has the \"initialize-fit-transform\" structure. For our purposes here, the most important argument upon initialization of the class is the `kind` argument, which specifies the particular connectivity measure (“correlation”, “partial correlation”, “tangent”, “covariance”, or “precision”). We will use the default values for the other arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvGhbsQwgjYD"
      },
      "outputs": [],
      "source": [
        "cm = ConnectivityMeasure(kind='correlation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9zxx_DhgjYD"
      },
      "source": [
        "Now, just as the `NiftiMasker` class, you need to call both the `fit` and `tranform` methods (or the `fit_transform` method for convenience) to actually estimate and return the $K \\times K$ connectivity matrix (or matrices, if you have multiple subjects). Importantly, the `fit` and `transform` (and `fit_transform`) methods take a *list* of numpy arrays (of shape $T \\times K$, where $T$ represents the number of time points and $K$ the number of ROIs) representing the individual subjects' data. These functions output a $N \\times K \\times K$ array, representing a $K \\times K$ connectivity matrix for all $N$ subjects.\n",
        "\n",
        "Here, we only have data from a single subject (contained in the `av_roi_signals` variable), so we'll supply the `fit_transform` function with a list containing a single array. As such, the output will be a $1 \\times K \\times K$ array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-k0X09dgjYD"
      },
      "outputs": [],
      "source": [
        "corr_mat = cm.fit_transform([av_roi_signals])  # input = list with single matrix\n",
        "print(\"corr_mat is a 2D array with shape: %s\" % (corr_mat.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQWN8aASgjYD"
      },
      "source": [
        "As we only a have single subject, let's \"squeeze\" out the singleton dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvCaXWy-gjYD"
      },
      "outputs": [],
      "source": [
        "corr_mat = corr_mat.squeeze()\n",
        "print(\"corr_mat now has the following shape: %s\" % (corr_mat.shape,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2fzZ2p-gjYD"
      },
      "source": [
        "Now, on the matrix (`corr_mat`) you could, for example, perform additional network-analyses or just visualize it, either the matrix itself or as a network of brain regions on top of a brain.\n",
        "\n",
        "First, we'll show how to plot the matrix. We'll use the `plotting.plot_matrix` function for this. Because the labels are barely readable with the default plot size, we'll create a Matplotlib figure beforehand and pass it to the plotting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlZ0SHszgjYD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "display = plotting.plot_matrix(\n",
        "    corr_mat,\n",
        "    labels=ho_maxprob_atlas['labels'][1:],\n",
        "    reorder='average',\n",
        "    figure=fig\n",
        ")\n",
        "\n",
        "# Increase the labels a bit\n",
        "display.axes.tick_params(axis='both', which='major', labelsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUiIXNagjYD"
      },
      "source": [
        "We can also create a visualization of the network of brain regions on top of a (transparent) background (MNI) brain using `plotting.plot_connectome`, or even an interactive version using `plotting.view_connectome`. Importantly, these functions need to know the (peak) MNI coordinates of the regions from the atlas that you used. Sometimes, these are included in the atlas (as a separate entry into the data dictionary, next to \"maps\" and \"labels\"), but if not, you can extract them using the `plotting.find_parcellation_cut_coords` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsP_h8A9gjYD"
      },
      "outputs": [],
      "source": [
        "coords = plotting.find_parcellation_cut_coords(ho_maxprob_atlas_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6xhhUs6gjYD"
      },
      "source": [
        "Now, let's create an interactive connectome plot. We'll set the `edge_threshold` to 90%, which will show only the edges with the 10% highest connectivity values (otherwise, the plot will become a bit cluttered):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzJ6egdKgjYD"
      },
      "outputs": [],
      "source": [
        "plotting.view_connectome(\n",
        "    corr_mat,\n",
        "    coords,\n",
        "    edge_threshold=\"90%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSv9EXmXgjYE"
      },
      "source": [
        "Note that the `view_connectome` function only plots the graph edges (i.e., the correlation values) on the left hemisphere (although the nodes represent bilateral ROIs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpMxC-hUgjYE"
      },
      "source": [
        "<div class='alert alert-warning'>\n",
        "    <b>ToDo</b>: Alright, a final ToDo! The connectivity data shows a lot of strong, positive correlations between average time series across ROIs. These results may, however, be confounded by shared noise sources, such as drift, motion, and respiratory or cardiac signals. Fortunately, this dataset also contains several confound regressors, that may improve the connectivity estimate.\n",
        "    \n",
        "Try the following:<br>\n",
        "\n",
        "<ul>\n",
        "    <li>\n",
        "        Using the <tt>NiftiLabelsMasker</tt> class, perform sensible preprocessing (e.g., high-pass filtering and smoothing) and regress out the confounds (in <tt>data['confounds']</tt> by passing it to the <tt>fit</tt> function;\n",
        "    </li>\n",
        "    <li>\n",
        "        Estimate the connectivity matrix again;\n",
        "    </li>\n",
        "    <li>\n",
        "        Visualize the connectivity matrix and interactive connectome plot again\n",
        "    </li>\n",
        "</ul>\n",
        "\n",
        "Note that few people seem to agree on the best ways to preprocess fMRI data for network analyses. If you ever want to do these type of analyses, we recommend checking the literature!\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f90700a96d953ed230f68e2f054a9c58",
          "grade": false,
          "grade_id": "cell-1f5cf7dccb8981ea",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "zqljT4YpgjYE"
      },
      "outputs": [],
      "source": [
        "# Implement the ToDo here\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKTasOthgjYE"
      },
      "source": [
        "## Concluding remarks\n",
        "We covered quite some functionality from the Nilearn packages, but definitely not all! It als contains some useful features for \"decoding\" analyses (in the [decoding](https://nilearn.github.io/modules/reference.html#module-nilearn.decoding) module, including a class to perform [Searchlight]()-based analysis and several classifiers for spatially-structured data such as fMRI data) amongst other things.\n",
        "\n",
        "We highly recommend you browse through the ample excellent tutorials and examples on the Nilearn [website](https://nilearn.github.io/), which cover other and more advanced usecases than discussed in this tutorial.\n",
        "\n",
        "That said, we hope that this tutorial helps you to get started with your analyses using Nilearn.<br>\n",
        "Happy hacking!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}